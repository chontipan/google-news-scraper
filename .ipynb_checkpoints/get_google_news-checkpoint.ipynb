{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "\n",
    "#get news for 3 months\n",
    "\n",
    "dict_links={}\n",
    "#get march\n",
    "for i in range(1,32):\n",
    "    googlenews = GoogleNews()\n",
    "    googlenews = GoogleNews(lang='en')\n",
    "    googlenews = GoogleNews(period='d')\n",
    "    timeRange=\"03/\"+str(i)+\"/2020\"\n",
    "    print(timeRange)\n",
    "#googlenews.setTimeRange('05/24/2020','05/24/2020')\n",
    "    googlenews.setTimeRange(timeRange,timeRange)\n",
    "    googlenews.search('Taiwan')\n",
    "    result = googlenews.result()\n",
    "    print(\"page 1:\",len(result))\n",
    "    links=[]\n",
    "    for r in result:\n",
    "        links.append(r[\"link\"])\n",
    "    dict_links[timeRange]=links\n",
    "\n",
    "    page=2\n",
    "    while(True):\n",
    "        googlenews = GoogleNews()\n",
    "        googlenews = GoogleNews(lang='en')\n",
    "        googlenews = GoogleNews(period='d')\n",
    "        googlenews.setTimeRange(timeRange,timeRange)\n",
    "        googlenews.search('Taiwan')\n",
    "        googlenews.clear()\n",
    "        googlenews.getpage(page)\n",
    "        result = googlenews.result()\n",
    "        print(\"page\",page,\":\",len(result))\n",
    "        if(len(result)==0):\n",
    "            break\n",
    "        page+=1\n",
    "\n",
    "    \n",
    "# # #get april\n",
    "for i in range(1,31):\n",
    "    googlenews = GoogleNews()\n",
    "    googlenews = GoogleNews(lang='en')\n",
    "    googlenews = GoogleNews(period='d')\n",
    "    timeRange=\"04/\"+str(i)+\"/2020\"\n",
    "    print(timeRange)\n",
    "    #googlenews.setTimeRange('05/24/2020','05/24/2020')\n",
    "    googlenews.setTimeRange(timeRange,timeRange)\n",
    "    googlenews.search('Taiwan')\n",
    "    result = googlenews.result()\n",
    "    print(\"page 1:\",len(result))\n",
    "    links=[]\n",
    "    for r in result:\n",
    "        links.append(r[\"link\"])\n",
    "    dict_links[timeRange]=links\n",
    "\n",
    "    page=2\n",
    "    while(True):\n",
    "        googlenews = GoogleNews()\n",
    "        googlenews = GoogleNews(lang='en')\n",
    "        googlenews = GoogleNews(period='d')\n",
    "        googlenews.setTimeRange(timeRange,timeRange)\n",
    "        googlenews.search('Taiwan')\n",
    "        googlenews.clear()\n",
    "        googlenews.getpage(page)\n",
    "        result = googlenews.result()\n",
    "        print(\"page\",page,\":\",len(result))\n",
    "        if(len(result)==0):\n",
    "            break\n",
    "        page+=1\n",
    "\n",
    "# #get till 25 may\n",
    "for i in range(1,26):\n",
    "    googlenews = GoogleNews()\n",
    "    googlenews = GoogleNews(lang='en')\n",
    "    googlenews = GoogleNews(period='d')\n",
    "    timeRange=\"05/\"+str(i)+\"/2020\"\n",
    "    print(timeRange)\n",
    "    #googlenews.setTimeRange('05/24/2020','05/24/2020')\n",
    "    googlenews.setTimeRange(timeRange,timeRange)\n",
    "    googlenews.search('Taiwan')\n",
    "    result = googlenews.result()\n",
    "    print(\"page 1:\",len(result))\n",
    "    links=[]\n",
    "    for r in result:\n",
    "        links.append(r[\"link\"])\n",
    "    dict_links[timeRange]=links\n",
    "\n",
    "    page=2\n",
    "    while(True):\n",
    "        googlenews = GoogleNews()\n",
    "        googlenews = GoogleNews(lang='en')\n",
    "        googlenews = GoogleNews(period='d')\n",
    "        googlenews.setTimeRange(timeRange,timeRange)\n",
    "        googlenews.search('Taiwan')\n",
    "        googlenews.clear()\n",
    "        googlenews.getpage(page)\n",
    "        result = googlenews.result()\n",
    "        print(\"page\",page,\":\",len(result))\n",
    "        if(len(result)==0):\n",
    "            break\n",
    "        page+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_links)\n",
    "total_urls=0\n",
    "for key, value in dict_links.items():\n",
    "    total_urls+=len(value)\n",
    "print(\"total urls:\",total_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter 4 news agencies\n",
    "matchers = ['https://www.taiwannews.com.tw','https://thetaiwantimes.com','https://www.taipeitimes.com','https://focustaiwan.tw']\n",
    "total_urls=0\n",
    "for key, value in dict_links.items():\n",
    "    matching = [s for s in value if any(xs in s for xs in matchers)]\n",
    "    dict_links[key]=matching\n",
    "    total_urls+=len(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"filtered urls:\",total_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#get content from urls\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "list_news = []\n",
    "for key, value in dict_links.items():\n",
    "    for url in value:\n",
    "        row={}\n",
    "        if \"https://focustaiwan.tw\" in url:\n",
    "            print(url)        \n",
    "            page = requests.get(url).text\n",
    "    \n",
    "            soup = BeautifulSoup(page, \"lxml\")\n",
    "            #mydivs = soup.findAll(\"div\", {\"class\": \"paragraph\"})\n",
    "            #mydivs = soup.findAll(\"p\")\n",
    "            text = \"\".join([p.text for p in soup.find_all(\"p\")])\n",
    "            #print(text)\n",
    "            #print(\"=\"*20)\n",
    "            row[\"date\"]=key\n",
    "            row[\"text\"]=text\n",
    "            row[\"url\"]=url\n",
    "            print(row)\n",
    "            list_news.append(row)\n",
    "            #content = soup.p\n",
    "            #date = soup.find(attrs={\"name\": \"DC.Date\"}).get('content')\n",
    "            #newstr = date.replace(\",\", \"\")\n",
    "            #date =datetime.datetime.strptime(newstr, '%B %d %Y').strftime('%Y%m%d')\n",
    "            #print(content)\n",
    "\n",
    "\n",
    "        else:\n",
    "            article = Article(url)\n",
    "            print(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp() \n",
    "            #print(\"title\",article.title) #prints the title of the article\n",
    "            #print(\"date:\",article.publish_date)\n",
    "            text=article.text\n",
    "            #print(\"desc:\",article.text)\n",
    "            #print(\"-\"*10)\n",
    "            #print(\"summary:\",article.summary)\n",
    "            #print(\"=\"*20)\n",
    "            row[\"date\"]=key\n",
    "            row[\"text\"]=text\n",
    "            row[\"url\"]=url\n",
    "            print(row)\n",
    "            list_news.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "import csv\n",
    "csv_columns = ['date','text','url']\n",
    "csv_file = \"news.csv\"\n",
    "\n",
    "try:\n",
    "    with open(csv_file, 'w', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns, lineterminator = '\\n')\n",
    "        writer.writeheader()\n",
    "        for data in list_news:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# index=1\n",
    "# urls=[]\n",
    "# for r in result:\n",
    "#     print(index)\n",
    "#     index+=1\n",
    "#     print(r[\"title\"])\n",
    "#     print(r[\"media\"])\n",
    "#     print(r[\"date\"])\n",
    "#     print(r[\"desc\"])\n",
    "#     print(r[\"link\"])\n",
    "#     urls.append(r[\"link\"])\n",
    "#     print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(urls))\n",
    "# urls_list = set(urls)\n",
    "# print(len(urls_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0'}\n",
    "# url = 'https://news.google.com/'\n",
    "# req = urllib.request.Request(url, headers=headers)\n",
    "# response = urllib.request.urlopen(req)\n",
    "# page = response.read()\n",
    "# print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from newspaper import Article\n",
    "\n",
    "# url = links[0]\n",
    "# article = Article(url)\n",
    "# print(url)\n",
    "# article.download()\n",
    "# article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article.authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = googlenews.gettext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = \"https://focustaiwan.tw/society/202005250004\"\n",
    "# article = Article(url)\n",
    "# print(url)\n",
    "# article.download()\n",
    "# article.parse()\n",
    "# article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.taipeitimes.com/News/front/archives/2020/05/25/2003736989\"\n",
    "# article = Article(url)\n",
    "# print(url)\n",
    "# article.download()\n",
    "# article.parse()\n",
    "# article.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.taiwannews.com.tw/en/news/3890529\"\n",
    "# article = Article(url)\n",
    "# print(url)\n",
    "# article.download()\n",
    "# article.parse()\n",
    "# article.text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"summary:\",article.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# news_url=\"https://news.google.cl/news/rss\"\n",
    "# rss_text=requests.get(news_url).text\n",
    "# soup_page=BeautifulSoup(rss_text,\"xml\")\n",
    "\n",
    "# def get_items(soup):\n",
    "#     for news in soup.findAll(\"item\"):\n",
    "#         s = BeautifulSoup(news.description.text, 'lxml')\n",
    "#         a = s.select('a')[-1]\n",
    "#         a.extract()         # extract lat 'See more on Google News..' link\n",
    "\n",
    "#         html = requests.get(news.link.text)\n",
    "#         soup_content = BeautifulSoup(html.text,\"lxml\")\n",
    "\n",
    "#         # perform basic sanitization:\n",
    "#         for t in soup_content.select('script, noscript, style, iframe, nav, footer, header'):\n",
    "#             t.extract()\n",
    "\n",
    "#         yield news.title.text.strip(), html.url, s.text.strip(), str(soup_content.select_one('body').text)\n",
    "\n",
    "# width = 80\n",
    "# for (title, url, shorttxt, content) in get_items(soup_page):\n",
    "#     title = '\\n'.join(textwrap.wrap(title, width))\n",
    "#     url = '\\n'.join(textwrap.wrap(url, width))\n",
    "#     shorttxt = '\\n'.join(textwrap.wrap(shorttxt, width))\n",
    "#     content = '\\n'.join(textwrap.wrap(textwrap.shorten(content, 1024), width))\n",
    "\n",
    "#     print(title)\n",
    "#     print(url)\n",
    "#     print('-' * width)\n",
    "#     print(shorttxt)\n",
    "#     print()\n",
    "#     print(content)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit33b66621aa3943cebcc08ffccb7bf69e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
